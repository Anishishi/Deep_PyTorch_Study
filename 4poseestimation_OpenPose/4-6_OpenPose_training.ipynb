{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import torchvision\n",
    "import torch.nn.init as init\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datapath_list(rootpath):\n",
    "    \"\"\"学習検証の画像データとアノテーションデータ，マスクデータへのファイルパスリストを作成\"\"\"\n",
    "    \n",
    "    #load JSON-file for annotation\n",
    "    json_path=osp.join(rootpath,'COCO.json')\n",
    "    with open(json_path) as data_file:\n",
    "        data_this=json.load(data_file)\n",
    "        data_json=data_this['root']\n",
    "        \n",
    "    #store index\n",
    "    num_sampels=len(data_json)\n",
    "    train_indexes=[]\n",
    "    val_indexes=[]\n",
    "    for count in range(num_sampels):\n",
    "        if data_json[count]['isValidation']!=0:\n",
    "            val_indexes.append(count)\n",
    "        else:\n",
    "            train_indexes.append(count)\n",
    "            \n",
    "    #store file paths to images\n",
    "    train_img_list=list()\n",
    "    val_img_list=list()\n",
    "    \n",
    "    for idx in train_indexes:\n",
    "        img_path=osp.join(rootpath,data_json[idx]['img_paths']) #'train2014/COCO_train2014_000000000036.jpg'\n",
    "        train_img_list.append(img_path)\n",
    "    for idx in val_indexes:\n",
    "        img_path=osp.join(rootpath,data_json[idx]['img_paths'])\n",
    "        val_img_list.append(img_path)\n",
    "        \n",
    "    #store paths to mask data\n",
    "    train_mask_list=[]\n",
    "    val_mask_list=[]\n",
    "    \n",
    "    for idx in train_indexes:\n",
    "        img_idx=data_json[idx]['img_paths'][-16:-4] #000000000036\n",
    "        anno_path=\"./data/mask/train2014/mask_COCO_train2014_\"+img_idx+'.jpg'\n",
    "        train_mask_list.append(anno_path)\n",
    "    for idx in val_indexes:\n",
    "        img_idx=data_json[idx]['img_paths'][-16:-4]\n",
    "        anno_path=\"./data/mask/val2014/mask_COCO_val2014_\"+img_idx+'.jpg'\n",
    "        val_mask_list.append(anno_path)\n",
    "        \n",
    "    #store annotation data\n",
    "    train_meta_list=list()\n",
    "    val_meta_list=list()\n",
    "    \n",
    "    for idx in train_indexes:\n",
    "        train_meta_list.append(data_json[idx])\n",
    "    for idx in val_indexes:\n",
    "        val_meta_list.append(data_json[idx])\n",
    "        \n",
    "    return train_img_list, train_mask_list, val_img_list, val_mask_list, train_meta_list, val_meta_list\n",
    "\n",
    "#データ処理のクラスとデータオーギュメンテーションのクラスをimportする\n",
    "from utils.data_augumentation import Compose,get_anno,add_neck,aug_scale,aug_rotate,aug_croppad,aug_croppad,aug_flip,remove_illegal_joint,\\\n",
    "Normalize_Tensor,no_Normalize_Tensor\n",
    "\n",
    "class DataTransform():\n",
    "    \"\"\"\n",
    "    画像とマスク，アノテーションの前処理クラス\n",
    "    学習時と推論時で異なる動作をする\n",
    "    学習時はdata augmentationをする\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.data_transform={\n",
    "            'train':Compose([\n",
    "                get_anno(), #store annotation from JSON to dict\n",
    "                add_neck(), #align anno data, add neck anno data\n",
    "                aug_scale(), #scaling\n",
    "                aug_rotate(), #rotation\n",
    "                aug_croppad(), #trimming\n",
    "                aug_flip(), #flip horizontal\n",
    "                remove_illegal_joint(), #remove the anno data out of img\n",
    "                #Normlize_Tensor()\n",
    "                no_Normalize_Tensor() #only in this chapter, no-Normalization\n",
    "            ]),\n",
    "            'val':Compose([\n",
    "                #omit in this textbook\n",
    "            ])\n",
    "        }\n",
    "        \n",
    "    def __call__(self,phase,meta_data,img,mask_miss):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        phase:'train'or'val' determine the mode of preprocess\n",
    "        \"\"\"\n",
    "        meta_data,img,mask_miss=self.data_transform[phase](meta_data,img,mask_miss)\n",
    "        \n",
    "        return meta_data,img,mask_miss\n",
    "    \n",
    "from utils.dataloader import get_ground_truth\n",
    "\n",
    "class COCOkeypointsDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    MSCOCOのCocokeypointsのdatasetを作成するクラス．pytorchのDatasetクラスを継承\n",
    "    \n",
    "    Attributes\n",
    "    img_list : list 画像のパスを格納したリスト\n",
    "    anno_list : list アノテーションへのパスを格納したリスト\n",
    "    pahse : 'train' or 'test'\n",
    "    transform : 前処理クラスのインスタンス\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,img_list,mask_list,meta_list,phase,transform):\n",
    "        super(COCOkeypointsDataset,self).__init__()\n",
    "        self.img_list=img_list\n",
    "        self.mask_list=mask_list\n",
    "        self.meta_list=meta_list\n",
    "        self.phase=phase\n",
    "        self.transform=transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        #画像の枚数を返す\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        img,heatmaps,heat_mask,pafs,paf_mask=self.pull_item(index)\n",
    "        return img,heatmaps,heat_mask,pafs,paf_mask\n",
    "\n",
    "    def pull_item(self,index):\n",
    "        #画像のtensor形式のデータ，アノテーション，マスクを取得する\n",
    "\n",
    "        #1. 画像読み込み\n",
    "        image_file_path=self.img_list[index]\n",
    "        img=cv2.imread(image_file_path) #[高さ][幅][色BGR]\n",
    "\n",
    "        #2. マスクとアノテーション読み込み\n",
    "        mask_miss=cv2.imread(self.mask_list[index])\n",
    "        meta_data=self.meta_list[index]\n",
    "\n",
    "        #3. 画像前処理\n",
    "        meta_data, img, mask_miss=self.transform(self.phase,meta_data,img,mask_miss)\n",
    "\n",
    "        #4. 正解アノテーションデータの取得\n",
    "        mask_miss_numpy=mask_miss.numpy().transpose((1,2,0))\n",
    "        heat_mask,heatmaps,paf_mask,pafs=get_ground_truth(meta_data,mask_miss_numpy)\n",
    "\n",
    "        #5. マスクデータはRGBが(1,1,1)か(0,0,0)なので，次元を落とす\n",
    "        #マスクデータはマスクされている場所は値が0,それ以外は1\n",
    "        heat_mask=heat_mask[:,:,:,0]\n",
    "        paf_mask=paf_mask[:,:,:,0]\n",
    "\n",
    "        #6. チャネルが最後尾にあるので順番を変える\n",
    "        # exp:paf_mask:torch.Size([46,46,38]) → torch.Size([38,46,46])\n",
    "        paf_mask=paf_mask.permute(2,0,1)\n",
    "        heat_mask=heat_mask.permute(2,0,1)\n",
    "        pafs=pafs.permute(2,0,1)\n",
    "        heatmaps=heatmaps.permute(2,0,1)\n",
    "\n",
    "        return img,heatmaps,heat_mask,pafs,paf_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenPoseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OpenPoseNet,self).__init__()\n",
    "        \n",
    "        #Feature module\n",
    "        self.model0=OpenPose_Feature()\n",
    "        \n",
    "        #Stage module\n",
    "        #PAFs (part affinity fields) side\n",
    "        self.model1_1=make_OpenPose_block('block1_1')\n",
    "        self.model2_1=make_OpenPose_block('block2_1')\n",
    "        self.model3_1=make_OpenPose_block('block3_1')\n",
    "        self.model4_1=make_OpenPose_block('block4_1')\n",
    "        self.model5_1=make_OpenPose_block('block5_1')\n",
    "        self.model6_1=make_OpenPose_block('block6_1')\n",
    "        \n",
    "        #confidence heatmap side\n",
    "        self.model1_2=make_OpenPose_block('block1_2')\n",
    "        self.model2_2=make_OpenPose_block('block2_2')\n",
    "        self.model3_2=make_OpenPose_block('block3_2')\n",
    "        self.model4_2=make_OpenPose_block('block4_2')\n",
    "        self.model5_2=make_OpenPose_block('block5_2')\n",
    "        self.model6_2=make_OpenPose_block('block6_2')\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #順伝搬定義\n",
    "        \n",
    "        #Feature module\n",
    "        out1=self.model0(x)\n",
    "        \n",
    "        #stage1\n",
    "        out1_1=self.model1_1(out1) #PAFs side\n",
    "        out1_2=self.model1_2(out1) #confidence heatmap side\n",
    "        \n",
    "        #stage2\n",
    "        out2=torch.cat([out1_1,out1_2,out1],1) #次元1のチャネルで結合\n",
    "        out2_1=self.model2_1(out2)\n",
    "        out2_2=self.model2_2(out2)\n",
    "        \n",
    "        #stage3\n",
    "        out3=torch.cat([out2_1,out2_2,out1],1)\n",
    "        out3_1=self.model3_1(out3)\n",
    "        out3_2=self.model3_2(out3)\n",
    "        \n",
    "        #stage4\n",
    "        out4=torch.cat([out3_1,out3_2,out1],1)\n",
    "        out4_1=self.model4_1(out4)\n",
    "        out4_2=self.model4_2(out4)\n",
    "        \n",
    "        #stage5\n",
    "        out5=torch.cat([out4_1,out4_2,out1],1)\n",
    "        out5_1=self.model5_1(out5)\n",
    "        out5_2=self.model5_2(out5)\n",
    "        \n",
    "        #stage6\n",
    "        out6=torch.cat([out5_1,out5_2,out1],1)\n",
    "        out6_1=self.model6_1(out6)\n",
    "        out6_2=self.model6_2(out6)\n",
    "        \n",
    "        #損失の計算用に各stageの結果を格納\n",
    "        saved_for_loss=[]\n",
    "        saved_for_loss.append(out1_1)\n",
    "        saved_for_loss.append(out1_2)\n",
    "        saved_for_loss.append(out2_1)\n",
    "        saved_for_loss.append(out2_2)\n",
    "        saved_for_loss.append(out3_1)\n",
    "        saved_for_loss.append(out3_2)\n",
    "        saved_for_loss.append(out4_1)\n",
    "        saved_for_loss.append(out4_2)\n",
    "        saved_for_loss.append(out5_1)\n",
    "        saved_for_loss.append(out5_2)\n",
    "        saved_for_loss.append(out6_1)\n",
    "        saved_for_loss.append(out6_2)\n",
    "        \n",
    "        #最終的なPAFsのout6_1とconfidence heatmapのout6_2，そして，損失計算用に各ステージでのPAFsとheatmapを格納したsaved_for_lossを出力\n",
    "        #out6_1 : torch.Size([minibatch, 38, 46, 46])\n",
    "        #out6_2 : torch.Size([minibatch, 19, 46, 46])\n",
    "        #saved_for_loss : [out1_1, out1_2, ... , out6_2]\n",
    "        \n",
    "        return (out6_1, out6_2), saved_for_loss\n",
    "    \n",
    "class OpenPose_Feature(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OpenPose_Feature,self).__init__()\n",
    "        \n",
    "        #VGG-19の最初10個の畳み込みを使用\n",
    "        #初めて実行する際はモデルの重みパラメタをダウンロードするため実行に時間がかかる\n",
    "        vgg19=torchvision.models.vgg19(pretrained=True)\n",
    "        model={}\n",
    "        model['block0']=vgg19.features[0:23] #vgg19の最初の10畳み込み層まで\n",
    "        \n",
    "        #残りは新たな畳み込み層を2つ用意\n",
    "        model['block0'].add_module(\"23\", torch.nn.Conv2d(\n",
    "            512,256,kernel_size=3,stride=1,padding=1))\n",
    "        model['block0'].add_module(\"24\", torch.nn.ReLU(inplace=True))\n",
    "        model['block0'].add_module(\"25\", torch.nn.Conv2d(\n",
    "            256,128,kernel_size=3,stride=1,padding=1))\n",
    "        model['block0'].add_module(\"26\", torch.nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.model=model['block0']\n",
    "        \n",
    "    def forward(self,x):\n",
    "        outputs=self.model(x)\n",
    "        return outputs\n",
    "    \n",
    "def make_OpenPose_block(block_name):\n",
    "    #コンフィギュレーション変数からOpenPoseのStageモジュールのblockを作成．nn.Moduleではなく，nn.Sequential\n",
    "    \n",
    "    #1.コンフィギュレーションの辞書変数blocksを作成し，ネットワークを生成させる\n",
    "    #最初に全パターンの辞書を用意し，引数block_nameのみを生成する\n",
    "    blocks={}\n",
    "    #stage1\n",
    "    blocks['block1_1']=[{'conv5_1_CPM_L1':[128,128,3,1,1]},\n",
    "                        {'conv5_2_CPM_L1':[128,128,3,1,1]},\n",
    "                        {'conv5_3_CPM_L1':[128,128,3,1,1]},\n",
    "                        {'conv5_4_CPM_L1':[128,512,1,1,0]},\n",
    "                        {'conv5_5_CPM_L1':[512,38,1,1,0]}]\n",
    "    \n",
    "    blocks['block1_2']=[{'conv5_1_CPM_L2':[128,128,3,1,1]},\n",
    "                        {'conv5_2_CPM_L2':[128,128,3,1,1]},\n",
    "                        {'conv5_3_CPM_L2':[128,128,3,1,1]},\n",
    "                        {'conv5_4_CPM_L2':[128,512,1,1,0]},\n",
    "                        {'conv5_5_CPM_L2':[512,19,1,1,0]}]\n",
    "    \n",
    "    #stage2-6\n",
    "    for i in range(2,7):\n",
    "        blocks['block%d_1' % i]=[\n",
    "            {'Mconv1_stage%d_L1' % i:[185,128,7,1,3]},\n",
    "            {'Mconv2_stage%d_L1' % i:[128,128,7,1,3]},\n",
    "            {'Mconv3_stage%d_L1' % i:[128,128,7,1,3]},\n",
    "            {'Mconv4_stage%d_L1' % i:[128,128,7,1,3]},\n",
    "            {'Mconv5_stage%d_L1' % i:[128,128,7,1,3]},\n",
    "            {'Mconv6_stage%d_L1' % i:[128,128,1,1,0]},\n",
    "            {'Mconv7_stage%d_L1' % i:[128,38,1,1,0]}\n",
    "        ]\n",
    "        blocks['block%d_2' % i]=[\n",
    "            {'Mconv1_stage%d_L2' % i:[185,128,7,1,3]},\n",
    "            {'Mconv2_stage%d_L2' % i:[128,128,7,1,3]},\n",
    "            {'Mconv3_stage%d_L2' % i:[128,128,7,1,3]},\n",
    "            {'Mconv4_stage%d_L2' % i:[128,128,7,1,3]},\n",
    "            {'Mconv5_stage%d_L2' % i:[128,128,7,1,3]},\n",
    "            {'Mconv6_stage%d_L2' % i:[128,128,1,1,0]},\n",
    "            {'Mconv7_stage%d_L2' % i:[128,19,1,1,0]}\n",
    "        ]\n",
    "        \n",
    "    #引数block_nameのコンフィギュレーション辞書を取り出す\n",
    "    cfg_dict=blocks[block_name]\n",
    "    \n",
    "    #2.コンフィギュレーション内容をリスト変数layersに格納\n",
    "    layers=[]\n",
    "    \n",
    "    #0番目から最後の層までを作成\n",
    "    for i in range(len(cfg_dict)):\n",
    "        for k, v in cfg_dict[i].items():\n",
    "            if 'pool' in k:\n",
    "                layers+=[nn.MaxPool2d(kernel_size=v[0],stride=v[1],padding=v[2])]\n",
    "            else:\n",
    "                conv2d=nn.Conv2d(in_channels=v[0],out_channels=v[1],kernel_size=v[2],stride=v[3],padding=v[4])\n",
    "                layers+=[conv2d,nn.ReLU(inplace=True)]\n",
    "                \n",
    "    #3.layersをSequentialにする\n",
    "    #ただし，最後にReLUはいらないのでその手前までを使用\n",
    "    net=nn.Sequential(*layers[:-1])\n",
    "    \n",
    "    #4.初期化関数の設定をし，畳み込み層を初期化する\n",
    "    def _initialize_weights_norm(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m,nn.Conv2d):\n",
    "                init.normal_(m.weight,std=0.01)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias,0.0)\n",
    "                    \n",
    "    net.apply(_initialize_weights_norm)\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MS COCOのファイルパスリストを作成\n",
    "train_img_list,train_mask_list, val_img_list, val_mask_list, train_meta_list, val_meta_list = make_datapath_list(rootpath=\"./data/\")\n",
    "\n",
    "#Dataset作成\n",
    "#本書ではデータ量の問題から，trainをvalで作成している\n",
    "\n",
    "train_dataset=COCOkeypointsDataset(val_img_list,val_mask_list,val_meta_list,phase='train',transform=DataTransform())\n",
    "\n",
    "#今回は簡易な学習とし検証データは作成しない\n",
    "#val_dataset=COCOkeypointsDataset(val_img_list,val_mask_list,val_meta_list,phase='val',transform=DataTransform())\n",
    "\n",
    "#DataLoader作成\n",
    "batch_size=32\n",
    "\n",
    "train_dataloader=data.DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "#val_dataloader=data.DataLoader(val_dataset,batch_size=batch_size,shuffle=False)\n",
    "\n",
    "#辞書型変数にまとめる\n",
    "#dataloaders_dict={\"train\":train_dataloader,\"val\":val_dataloader}\n",
    "dataloaders_dict={\"train\":train_dataloader,\"val\":None}\n",
    "\n",
    "net=OpenPoseNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#損失関数の設定\n",
    "class OpenPoseLoss(nn.Module):\n",
    "    #OpenPoseの損失関数のクラスです\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(OpenPoseLoss,self).__init__()\n",
    "        \n",
    "    def forward(self,saved_for_loss,heatmap_target,heat_mask,paf_target,paf_mask):\n",
    "        \"\"\"\n",
    "        損失関数の計算\n",
    "        \n",
    "        Parameters\n",
    "        saved_for_loss : OpenPoseNetの出力（リスト）\n",
    "        heatmap_target : [num_batch,19,46,46]\n",
    "        heatmap_mask : [num_batch,19,46,46] heatmap画像のmask\n",
    "        paf_target : [num_batch,38,46,46] 正解のPAFのアノテーション情報\n",
    "        paf_mask : [num_batch,38,46,46] PAF画像のmask\n",
    "        \n",
    "        Returns\n",
    "        loss : tensor 損失値\n",
    "        \"\"\"\n",
    "        \n",
    "        total_loss=0\n",
    "        #ステージごとに計算\n",
    "        for j in range(6):\n",
    "            \n",
    "            #PAFsとheatmapsにおいて，マスクされている部分(paf_mask=0など)は無視する\n",
    "            #PAFs\n",
    "            pred1=saved_for_loss[2*j]*paf_mask\n",
    "            gt1=paf_target.float()*paf_mask\n",
    "            \n",
    "            #heatmaps\n",
    "            pred2=saved_for_loss[2*j+1]*heat_mask\n",
    "            gt2=heatmap_target.float()*heat_mask\n",
    "            \n",
    "            total_loss += F.mse_loss(pred1,gt1,reduction='mean') + F.mse_loss(pred2,gt2,reduction='mean')\n",
    "            \n",
    "        return total_loss\n",
    "    \n",
    "criterion=OpenPoseLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=optim.SGD(net.parameters(),lr=1e-2,momentum=0.9,weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデルを学習させる関数を作成\n",
    "\n",
    "def train_model(net,dataloaders_dict,criterion,optimizer,num_epochs):\n",
    "    \n",
    "    #GPUが使えるか確認\n",
    "    device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"使用デバイス：\",device)\n",
    "    \n",
    "    #ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "    \n",
    "    #ネットワークがある程度固定であれば高速化させる\n",
    "    torch.backends.cudnn.benchmark=True\n",
    "    \n",
    "    #画像の枚数\n",
    "    num_train_imgs=len(dataloaders_dict[\"train\"].dataset)\n",
    "    batch_size=dataloaders_dict[\"train\"].batch_size\n",
    "    \n",
    "    #イテレーションカウンタをセット\n",
    "    iteration=1\n",
    "    \n",
    "    #epoch-loop\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        #開始時刻\n",
    "        t_epoch_start=time.time()\n",
    "        t_iter_start=time.time()\n",
    "        epoch_train_loss=0.0 #epochの損失和\n",
    "        epoch_val_loss=0.0 #epochの損失和\n",
    "        \n",
    "        print('-------------------')\n",
    "        print('Epoch {}/{}'.format(epoch+1,num_epochs))\n",
    "        print('-------------------')\n",
    "        \n",
    "        #epochごとの訓練と検証のループ\n",
    "        for phase in ['train','val']:\n",
    "            if phase=='train':\n",
    "                net.train()\n",
    "                optimizer.zero_grad()\n",
    "                print(' (train) ')\n",
    "            #今回は検証はスキップ\n",
    "            else:\n",
    "                continue\n",
    "                #net.eval()\n",
    "                #print('-------------------')\n",
    "                #print(' (val) ')\n",
    "                \n",
    "            #データローダからミニバッチずつ取り出すループ\n",
    "            for imges,heatmap_target,heat_mask,paf_target,paf_mask in dataloaders_dict[phase]:\n",
    "                #ミニバッチサイズが1だと，バッチノーマライゼーションでエラーになる\n",
    "                if imges.size()[0]==1:\n",
    "                    continue\n",
    "                    \n",
    "                #GPUが使えるならGPUにデータを送る\n",
    "                imges=imges.to(device)\n",
    "                heatmap_target=heatmap_target.to(device)\n",
    "                heat_mask=heat_mask.to(device)\n",
    "                paf_target=paf_target.to(device)\n",
    "                paf_mask=paf_mask.to(device)\n",
    "                \n",
    "                #optimizerを初期化\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                #calc forward\n",
    "                with torch.set_grad_enabled(phase=='train'):\n",
    "                    #(out6_1,out6_2)は使わないので＿で代替\n",
    "                    _,saved_for_loss=net(imges)\n",
    "                    \n",
    "                    loss=criterion(saved_for_loss,heatmap_target,heat_mask,paf_target,paf_mask)\n",
    "                    \n",
    "                    #訓練時はback-prop\n",
    "                    if phase=='train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        if(iteration%10==0):\n",
    "                            t_iter_finishi=time.time()\n",
    "                            duration=t_iter_finishi-t_iter_start\n",
    "                            print('イテレーション {} || Loss: {:.4f} || 10iter: {:.4f} sec.'.format(\n",
    "                                iteration,loss.item()/batch_size,duration))\n",
    "                            t_iter_start=time.time()\n",
    "                        \n",
    "                        epoch_train_loss+=loss.item()\n",
    "                        iteration+=1\n",
    "                    \n",
    "                    #検証時\n",
    "                    #else:\n",
    "                        #epoch_val_loss+=loss.item()\n",
    "                        \n",
    "        #epochのphaseごとのlossと正解率\n",
    "        t_epoch_finishi=time.time()\n",
    "        print('-------------------')\n",
    "        print('epoch {} || Epoch_TRAIN_Loss:{:.4f} ||Epoch__VAL_Loss:{:.4f}'.format(\n",
    "            epoch+1,epoch_train_loss/num_train_imgs, 0))\n",
    "        print('timer: {:.4f} sec'.format(t_epoch_finishi-t_epoch_start))\n",
    "        t_epoch_start=time.time()\n",
    "        \n",
    "    #最後のネットワークを保存\n",
    "    torch.save(net.state_dict(), './weights/openpose_net_'+str(epoch+1)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用デバイス： cuda:0\n",
      "-------------------\n",
      "Epoch 1/2\n",
      "-------------------\n",
      " (train) \n",
      "イテレーション 10 || Loss: 0.0093 || 10iter: 69.2442 sec.\n",
      "イテレーション 20 || Loss: 0.0084 || 10iter: 53.8389 sec.\n",
      "イテレーション 30 || Loss: 0.0070 || 10iter: 44.0771 sec.\n",
      "イテレーション 40 || Loss: 0.0059 || 10iter: 44.5049 sec.\n",
      "イテレーション 50 || Loss: 0.0049 || 10iter: 40.5896 sec.\n",
      "イテレーション 60 || Loss: 0.0041 || 10iter: 36.9888 sec.\n",
      "イテレーション 70 || Loss: 0.0039 || 10iter: 34.4608 sec.\n",
      "イテレーション 80 || Loss: 0.0033 || 10iter: 35.2298 sec.\n",
      "イテレーション 90 || Loss: 0.0028 || 10iter: 34.0990 sec.\n",
      "イテレーション 100 || Loss: 0.0025 || 10iter: 34.1013 sec.\n",
      "イテレーション 110 || Loss: 0.0023 || 10iter: 33.2162 sec.\n",
      "イテレーション 120 || Loss: 0.0021 || 10iter: 35.2386 sec.\n",
      "イテレーション 130 || Loss: 0.0021 || 10iter: 35.0580 sec.\n",
      "イテレーション 140 || Loss: 0.0018 || 10iter: 34.1989 sec.\n",
      "イテレーション 150 || Loss: 0.0018 || 10iter: 33.7657 sec.\n",
      "-------------------\n",
      "epoch 1 || Epoch_TRAIN_Loss:0.0043 ||Epoch__VAL_Loss:0.0000\n",
      "timer: 614.5134 sec\n",
      "-------------------\n",
      "Epoch 2/2\n",
      "-------------------\n",
      " (train) \n",
      "イテレーション 160 || Loss: 0.0017 || 10iter: 21.4691 sec.\n",
      "イテレーション 170 || Loss: 0.0017 || 10iter: 30.6227 sec.\n",
      "イテレーション 180 || Loss: 0.0016 || 10iter: 30.9177 sec.\n",
      "イテレーション 190 || Loss: 0.0012 || 10iter: 29.8617 sec.\n",
      "イテレーション 200 || Loss: 0.0016 || 10iter: 29.7432 sec.\n",
      "イテレーション 210 || Loss: 0.0016 || 10iter: 30.1443 sec.\n",
      "イテレーション 220 || Loss: 0.0014 || 10iter: 28.9958 sec.\n",
      "イテレーション 230 || Loss: 0.0016 || 10iter: 29.2581 sec.\n",
      "イテレーション 240 || Loss: 0.0016 || 10iter: 30.6896 sec.\n",
      "イテレーション 250 || Loss: 0.0012 || 10iter: 29.0903 sec.\n",
      "イテレーション 260 || Loss: 0.0011 || 10iter: 29.4543 sec.\n",
      "イテレーション 270 || Loss: 0.0014 || 10iter: 31.4125 sec.\n",
      "イテレーション 280 || Loss: 0.0015 || 10iter: 29.6732 sec.\n",
      "イテレーション 290 || Loss: 0.0013 || 10iter: 30.2480 sec.\n",
      "イテレーション 300 || Loss: 0.0015 || 10iter: 30.1340 sec.\n",
      "-------------------\n",
      "epoch 2 || Epoch_TRAIN_Loss:0.0015 ||Epoch__VAL_Loss:0.0000\n",
      "timer: 463.2200 sec\n"
     ]
    }
   ],
   "source": [
    "#学習・検証\n",
    "num_epochs=2\n",
    "train_model(net,dataloaders_dict,criterion,optimizer,num_epochs=num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
