{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パッケージのimport\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# フォルダ「utils」にある関数matchを記述したmatch.pyからimport\n",
    "from utils.match import match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBoxLoss(nn.Module):\n",
    "    #SSDの損失関数のクラス\n",
    "    \n",
    "    def __init__(self,jaccard_thresh=0.5, neg_pos=3, devide='cpu'):\n",
    "        super(MultiBoxLoss,self).__init__()\n",
    "        self.jaccard_thresh=jaccard_thresh #0.5 関数matchのjaccard係数の閾値\n",
    "        self.negpos_ratio=neg_pos #3:1 HardNegativeMiningの負と正の比率(背景クラスのデータを全て学習に使うわけではない)\n",
    "        self.device=device\n",
    "        \n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        損失関数の計算\n",
    "        \n",
    "        parameters\n",
    "        predictions:SSD netの訓練時の出力(tuple)\n",
    "            (loc=torch.Size([num_batch, 8732, 4]), conf=torch.Size([num_batch, 8732, 21]), dbox_list=torch.Size([8732,4]))\n",
    "        targets:[num_batch, num_objs, 5] 5は正解のアノテーション情報[xmin, ymin, xmax, ymax, label_ind]\n",
    "        \n",
    "        returns\n",
    "        loss_l:tensor locの損失値\n",
    "        loss_c:tensor confの損失値\n",
    "        \"\"\"\n",
    "        \n",
    "        #SSDモデルの出力がタプルになっているので個々にバラす\n",
    "        loc_data, conf_data, dbox_list = predictions\n",
    "        \n",
    "        #要素数を把握\n",
    "        num_batch = loc_data.size(0)\n",
    "        num_dbox = loc_data.size(1)\n",
    "        num_classes = loc_data.size(2)\n",
    "        \n",
    "        #損失の計算に使用するものを格納する変数作成\n",
    "        #conf_t_label:各DBoxに一番近い正解のBBoxのラベルを格納する\n",
    "        #loc_t:各DBoxに一番近い正解のBBoxの位置情報を格納させる\n",
    "        conf_t_label = torch.LongTensor(num_batch, num_dbox).to(self.device)\n",
    "        loc_t = torch.Tensor(num_batch, num_dbox, 4).to(self.device)\n",
    "        \n",
    "        #loc_tとconf_t_labelに，DBoxと正解アノテーションtargetsをmatchさせた結果を上書きする\n",
    "        for idx in range(num_batch):\n",
    "            \n",
    "            #現在のミニバッチの正解アノテーションのBBoxとラベルを取得\n",
    "            truths = targets[idx][:,:-1].to(self.device) #BBox\n",
    "            #ラベル[物体1のラベル，物体2のラベル,...]\n",
    "            labels = targets[idx][:,-1].to(self.device)\n",
    "            \n",
    "            #デフォルトボックスを新たな変数で用意\n",
    "            dbox = dbox_list.to(self.device)\n",
    "            \n",
    "            #関数matchを実行し，loc_t,conf_t_labelの内容を更新する\n",
    "            #(詳細)参考書p106のあたり\n",
    "            variance=[0.1,0.2]\n",
    "            #このvarianceはDBoxからBBoxに補正計算する際に使用する式の係数\n",
    "            match(self.jaccard_thresh, truths, dbox, variance, labels, loc_t, conf_t_label, idx)\n",
    "            \n",
    "        #位置損失:loss_lを計算--------------------\n",
    "        #smoothL1関数を用いる．ただし，物体を発見したDBoxのオフセットのみを用いる\n",
    "        #物体を検出したBBoxを取り出すマスクを作成\n",
    "        pos_mask = conf_t_label > 0\n",
    "        \n",
    "        #pos_maskをloc_dataのサイズに変形\n",
    "        pos_idx = pos_mask.unsqueeze(pos_mask.dim()).expand_as(loc_data)\n",
    "        \n",
    "        #Positive DBoxのloc_dataと，教師データloc_tを取得\n",
    "        loc_p = loc_data[pos_idx].view(-1,4)\n",
    "        loc_t = loc_t[pos_idx].view(-1,4)\n",
    "        \n",
    "        #物体を発見したpositive dboxのオフセット情報loc_tの損失(誤差)を計算\n",
    "        loss_l = F.smooth_l1_loss(loc_p, loc_t, reduction='sum')\n",
    "        \n",
    "        #クラス予測の損失:loss_cを計算---------------------------\n",
    "        #交差エントロピー誤差関数で損失を計算する．ただし，背景クラスが正解であるDBoxが圧倒的に多いのでHard Negative Miningで\n",
    "        #物体発見dboxと背景クラスdboxの比が1:3になるようにする\n",
    "        #そこで背景クラスdboxと予測したもののうち，損失が小さいものはクラス予測の損失から除く\n",
    "        batch_conf = conf_data.view(-1,4)\n",
    "        \n",
    "        #クラス予測の損失を計算(reduction='none'にして，和を取らず，次元を潰さない)\n",
    "        loss_c = F.cross_entropy(batch_conf, conf_t_label.view(-1), reduction='none')\n",
    "        \n",
    "        #これからnegative dboxのうちHard Negative Miningで抽出するものを求めるマスクを作成する\n",
    "        \n",
    "        #物体発見したPositive DBoxの損失を0にする．\n",
    "        num_pos = pos_mask.long().sum(1, keepdim=True) #ミニバッチごとの物体クラス予測の数\n",
    "        loss_c = loss_c.view(num_batch, -1)\n",
    "        loss_c[pos_mask] = 0\n",
    "        \n",
    "        #Hard Negative Mining. 各DBoxの損失の大きさloss_cの順位であるidx_rankを求める\n",
    "        _,loss_idx = loss_c.sort(1, descending=True)\n",
    "        _,idx_rank = loss_idx.sort(1)\n",
    "        \n",
    "        #背景のDBoxの数num_negを決める．\n",
    "        num_neg = torch.clamp(num_pos*self.negpos_ratio, max=num_dbox)\n",
    "        \n",
    "        #idx_rankは各DBoxの損失の大きさが上から何番目なのかが入っている\n",
    "        #背景のDBoxの数num_negより，順位が低い(すなわち損失が多い)DBoxを取るマスク\n",
    "        neg_mask = idx_rank < (num_neg).expand_as(idx_rank)\n",
    "        \n",
    "        #終了\n",
    "        \n",
    "        #マスクの形を整形し，conf_dataに合わせる\n",
    "        #pos_mask:torch.Size([num_batch, 8732])→pos_idx_mask:torch.Size([num_batch, 8732, 21])\n",
    "        pos_idx_mask = pos_mask.unsqueeze(2).expand_as(conf_data)\n",
    "        neg_idx_mask = neg_mask.unsqueeze(2).expand_as(conf_data)\n",
    "        \n",
    "        #conf_hnm.size([num_pos+num_neg, 21])\n",
    "        conf_hnm = conf_data[(pos_idx_mask+neg_idx_mask).gt(0)].view(-1, num_classes)\n",
    "        \n",
    "        #同様に教師データであるconf_t_labelからpos,negだけを取り出し，conf_label_hnm\n",
    "        conf_t_label_hnm = conf_t_label[(pos_mask+neg_mask).gt(0)]\n",
    "        \n",
    "        #confidenceの損失を計算\n",
    "        loss_c = F.cross_entropy(conf_hnm, conf_t_label_hnm, reduction='sum')\n",
    "        \n",
    "        #物体を発見したBBoxの数N(全ミニバッチの合計)で損失を割る\n",
    "        N = num_pos.sum()\n",
    "        loss_l /= N\n",
    "        loss_c /= N\n",
    "        \n",
    "        return loss_l, loss_c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
